# Double Dominoes

This is my first project and zero time went into design so please forgive my spaghetti code...

If you want an in-depth analysis of gaming search algorithms check out "Multiplayer Games: Algorithms and Approaches, Sturtevant, 2003"

--> https://webdocs.cs.ualberta.ca/~nathanst/papers/multiplayergamesthesis.pdf

## Introduction
For this project I simulated games of Double Dominoes and created three AIs to play against one another. Doing this I learned a bit about search algorithms and will share that with you. The first thing to consider when implementing a search algorithm in a game is the evaluation function. This function is used in many search algorithms and will return a value range based on the current state of the board. Essentially, the goal is to search a tree (every node layer is a turn and every branch a possible move) and find a path that reaches a node which has a good evaluation for that player. For this to work however the same evaluation function needs to run when it's an opponent’s turn on the tree in order to determine their possible scores as well. The challenge for certain games is limited information, for example card games compared to chess. In chess we are given exactly the same information about a board as our opponent, whereas in a card game (or in this case dominoes) the only information we have is what pieces we have, how many pieces our opponent has and what has already been played. So, it is a bit tricky to come up with an effective evaluation function as domino scores are highly depended on the value of a tile. The simple fact remains for non-perfect information games the evaluation function needs to work for all players not only the player whose information we have. 

Next thing to consider is how to search the tree. There is no one size fits all and every game will have its own requirements for an algorithm. In a game such as dominoes one major issue is that high up in the tree (few moves played), the evaluation function will return very similar results for all the possible moves. It is only when you get to the leaves of the tree (terminal nodes, i.e. last moves) that the function will start to return larger differences in evaluation. This is why algorithms such as MaxN search the whole depth of the tree. The issue then lies with time and complexity. A tree grows exponentially with the number of nodes equalling number of branches to the power of depth, which in the case of three player dominoes that can easily go up to 5^32 in the first few moves. A solution is to not search certain branches which is called pruning and depending on the evaluation function and the nature of the game can lead to some success.

Pruning is technically always less optimal at finding the best move but can be more effective as given less time a larger number of leaf nodes can be searched. When dealing with large numbers this can be beneficial, this however goes back to the evaluation function. In order to prune a branch, the node score cannot rely on the terminal node evaluation as this would simply be a regular MaxN search but rather needs the evaluation function to return a value that signals a loss in score. In games where the points are only added up at the end and we don't know what our opponent has in their hand this becomes challenging. As mentioned previously for this game my evaluation function relies heavily on the counting of tiles and the knowledge of tiles already played. Meaning that the evaluation result stays consistent until the later stage of the game. Thus, when I tried pruning using various algorithms such as Speculative Pruning, Approximate Deep Search and Best-Reply Search there was a drop in performance win wise as pruned branches were pruned based on their evaluation relative to nodes on a similar level (which would always be similar due to the inaccuracy of the evaluation function).

Considering the above, my implementation is quite a simple one where I set a node search limit on the MaxN algorithm. This results in pseudo-random play in the early stages of the game and in later stages a more accurate decision-making process. Details are discussed further down. One last thing to mention is that the other search algorithms are a bit more complicated to implement than plain MaxN so the drop in win percentage of these algorithms could possibly be due to incorrect implementation and not the logic described above. I've included a text file with pseudo-code of the other algorithms called "MaxN Algorithms".

## Game Rules
Double Dominoes - also known as Mexican Train - is like dominoes with a few added rules. Firstly, the numbers go up to twelve. Each player has their own "Train" where they can play their tiles. The game is started with the double 12 in the middle. Players take turns building their trains, if they can't play their train is "Opened" meaning other players can play on their train. If a player plays on their own "Open" train it is closed again and only they can play on it. Additionally, there is a "Sauce" train which is always open and anyone can play on. If a player can't play on their own train but can play on another train their train remains closed. Lastly if a double ("Gate") is played (e.g. 11-11 on a 8-11) then play can only resume once another tile has been played on the double (e.g. 11-6 on the 11-11), players take turns trying to play on the double and can't play on any other trains until the "Gate" is opened, the person who played the double is the first person who needs to attempt to open it, if a player cannot open the gate they must pick up. Round ends when a player has finished their tiles or no one can play and there are no more tiles to pick up. A players score per round is the sum of numbers on their left-over tiles, play continues until a player hits 100, then whoever has the lowest score wins.

## Results
- With basic strategy after 10 000 games AI_2 wins 32% of the time against three other AI_1 players which is somewhat above the random distribution of 25%. Against 5 other AI_1 players it won 20% (16% random distribution), against 2 other AI_1's it won 47% (33% random distribution) and against only 1 AI_1 it won 70% of the time.
- At 3000 node search limit after 1000 games AI_3 played equally to two random players, so node depth needs to be higher or a better solution to node sorting and searching needs to be implemented. 

## The AI
### AI_1 
- Plays randomly. I did this to create a benchmark to compare with.
### AI_2 
- Plays somewhat strategically by building sequences in its hand and prioritizing where it plays. Firstly it will check for "Interrupting Moves", a move that limits other players ability to play (currently it only calls this function if the move would mean other players can't play at all, I also played around with playing tiles that somewhat limit other player moves but after running some simulations noticed it barely made a difference since it is only until the last few turns when this tactic actually makes a difference and if it takes priority over the sequence moves then it doesn't improve winning chances). Next it prioritizes playing on its own train using a sequence from its hand (The "best" sequence is selected from all possible generated sequences, the chosen sequence factors in sequence length, number of doubles in sequence and the total value of all the tiles in the sequence). Then it will prioritize playing high value tiles on the "sauce" train, and as a last resort it will play on other players trains.
### AI_3
- Apart from the search algorithm there is a "Terminal()" function and a "Find_Best_Move()" function. Terminal works by checking whether a player has finished their tiles or alternatively checking if all tiles have been picked up and no one can play. The function to find the best move works by creating a child node with a new sate for every possible move and then running the MaxN algorithm with a unique node as an input parameter. AI_3 currently uses the MaxN Algorithm with a search limit applied. The main reason for the search limit was so that it would run fast enough to do a test of win distribution vs the other AI's. Choice of this algorithm is explained in the introduction and essentially the search limit results in it playing pseudo randomly until the later stages of the game when search width/depth is not an issue as there are fewer moves to evaluate. The current search depth is set to 5000 which is relatively low and a precise number could be found with more testing. Ultimately it is a balance between performance play wise and time wise, and finding the perfect balance for a specific game is a rabbit whole I've explored enough of... At least for Double Dominoes.

## Program
### 'Node_Class.py'
- This class allows for the search algorithms to build a search tree. It does this by storing an array of children in the current node. Each node has some built in functionality to help with searching. 'Give_Options()' returns all the available moves that the current node has, which in the case of an opponent player will return a list of theoretically possible moves since the current player has no knowledge of the pieces the opponent has. 'Evalaute_Board()' returns a value of the board in a range from 0 - 1, where 1 is losing and 0 is winning. It does this by calculating the number of points not played and averaging it across how many pieces a player has left. It then divides that number by the total amount of points left overall, which will always be a fraction of one. For the current player it could be more refined, but this wouldn't work as a search algorithm needs to be able to evaluate an opponent’s "score" without knowing what pieces they have. 'New_State()' returns a new board based of a move made by a player, this is done so that a search algorithm can assign new evaluations when moving down the tree, it also does this when a player picks up. Lastly it can also 'Create_Children_Nodes()' for the search algorithm, it does this by taking all possible moves and creating a new state (node) for each move and returning a list of node objects.
- Each node stores the current board, a score for the board, the current player and children nodes.
- When implementing other algorithms such as Speculative Pruning more functionality needed to be added, things like parent nodes, "best" score and also a slightly different implementation (note that I removed these as I decided to stick with the MaxN algorithm)

### "Main.py" --imports--> "Simulator.py"
- Main allows for changing number of players and inputting the number of simulations to run. It also keeps track of player wins as that is what Simulator returns.

### "Simulator.py" --imports--> "Functions.py", "AI_'1|2|3|4'.py" (AI_2 imports the sequence class to store sequence objects)
- "Simulator.py" has various functions to keep track of scores and has the Start_Game() function which will run one game through its rounds. It calls various functions from "Functions.py" and the AI files. Simulator accounts for things like rotating player turns, checking for game end conditions, checking for closed gates and other basics for gameplay to work. Note that there is a match case used in Start_Game() by which it is possible to select which AI will use what method of play. It also has various comments which can be uncommented to print data to the console.

### "Functions.py" ----> Classes (Players, Pile, Trains)
- Functions uses these classes and initiates them when called by the simulator so the data is all passed through and "Simulator.py" doesn't need to import any classes. When calling the Closed_Gate() and calling the AI moves all the data is passed into those functions and returned back to the simulator, there is probably a way to access the data in a more efficient way but I haven’t looked into it. 

## Notes

- I initially decided to use numpy instead of lists to store tiles. I thought the difference in speed would probably be negligible and lists would've been a lot simpler, but I wanted to try and get familiar with the library. I only later discovered that numpy is very slow when appending things to an array as it creates a new copy every time. Lists is also a lot easier to index tiles as a list of tuples can be used.

- To compare the AI's I had them initially play 10 000 games after each change I made. For some reason when there are only 2 players the simulation hangs at certain points, haven't investigated why that is, seems like it gets stuck in some loops.

- The sequence creator functions are a bit of a mess but it seems to work and don't think it's worth optimizing as it doesn't seem to significantly slow the program.
